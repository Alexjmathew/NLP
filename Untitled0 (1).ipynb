{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "How many words (tokens) are there in the given text."
      ],
      "metadata": {
        "id": "v5opegyoL0qB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zYJj-dNKnPQ",
        "outputId": "20a646a0-8dbd-4302-aeed-8d9f450d6df4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of words (tokens) in the text is: 19\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "text = \"This is a sample text to count the number of words. It contains multiple sentences and punctuation.\"\n",
        "words = nltk.word_tokenize(text)\n",
        "num_words = len(words)\n",
        "print(\"The number of words (tokens) in the text is:\", num_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " How many different words (types) are there in the given text"
      ],
      "metadata": {
        "id": "bW7jt-eML-um"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "text = \"This is a sample text with some repeated words to demonstrate counting unique words.\"\n",
        "words = nltk.word_tokenize(text)\n",
        "unique_words = set(words)\n",
        "num_unique_words = len(unique_words)\n",
        "\n",
        "print(\"The number of unique words (types) in the text is:\", num_unique_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mwV1PLWMjM6",
        "outputId": "6f3d7585-c62f-46fe-8e3e-da4356d064c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of unique words (types) in the text is: 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How many times does the word the occur in the text"
      ],
      "metadata": {
        "id": "JbsVqyBnMAHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "text = \"This is the text we'll use to count how many times the word 'the' appears. The more the merrier!\"\n",
        "words = nltk.word_tokenize(text)\n",
        "count = words.count(\"the\")\n",
        "\n",
        "print(\"The word 'the' appears\", count, \"times in the text.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oHYnCDdMmto",
        "outputId": "e82c856a-f9e7-4e27-d1e2-1f64cda7d00f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The word 'the' appears 3 times in the text.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is this as a percentage of all the words in the text?"
      ],
      "metadata": {
        "id": "ulCg4_tYMGNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "text = \"This is the text we'll use to count how many times the word 'the' appears. The more the merrier!\"\n",
        "target_word = \"the\"\n",
        "words = nltk.word_tokenize(text)\n",
        "word_count = words.count(target_word)\n",
        "total_words = len(words)\n",
        "percentage = (word_count / total_words) * 100\n",
        "\n",
        "print(\"The word '\" + target_word + \"' appears\", word_count, \"times in the text.\")\n",
        "print(\"This is\", percentage, \"% of all the words in the text.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBUrVCpxMqLg",
        "outputId": "301385c0-4579-4e33-8f79-e5fd27eb7b1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The word 'the' appears 3 times in the text.\n",
            "This is 13.043478260869565 % of all the words in the text.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXPERIMENT 4**"
      ],
      "metadata": {
        "id": "6KTACH1hM3v5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6yf7YEP9M7yl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "text = \"This is a sample text to demonstrate text preprocessing techniques.\"\n",
        "tokens = nltk.word_tokenize(text)\n",
        "print(\"Tokenized words:\", tokens)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_removed = [word for word in tokens if word.lower() not in stop_words]\n",
        "print(\"Built-in stop words removed:\", stop_removed)\n",
        "custom_stop = {\"text\", \"sample\"}\n",
        "stop_words.update(custom_stop)\n",
        "stop_removed_new = [word for word in tokens if word.lower() not in stop_words]\n",
        "print(\"Custom stop words removed:\", stop_removed_new)\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stem_removed = [stemmer.stem(word) for word in stop_removed_new]\n",
        "print(\"Stemmed words:\", stem_removed)\n",
        "\n",
        "text2 = \"Running is fun and playing games is enjoyable.\"\n",
        "tokens2 = nltk.word_tokenize(text2)\n",
        "pos_tags = nltk.pos_tag(tokens2)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN  # Default to noun\n",
        "\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
        "\n",
        "print(f\"Original text: {text2}\")\n",
        "print(f\"Lemmatized text: {' '.join(lemmatized_words)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fKbdn5cNP6B",
        "outputId": "4e4a939c-22da-4564-8c48-c3433e1e8958"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized words: ['This', 'is', 'a', 'sample', 'text', 'to', 'demonstrate', 'text', 'preprocessing', 'techniques', '.']\n",
            "Built-in stop words removed: ['sample', 'text', 'demonstrate', 'text', 'preprocessing', 'techniques', '.']\n",
            "Custom stop words removed: ['demonstrate', 'preprocessing', 'techniques', '.']\n",
            "Stemmed words: ['demonstr', 'preprocess', 'techniqu', '.']\n",
            "Original text: Running is fun and playing games is enjoyable.\n",
            "Lemmatized text: Running be fun and play game be enjoyable .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXPERIMENT 5**"
      ],
      "metadata": {
        "id": "0Zx7hdqxNg6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Write a python program to replace words with its synonyms and words matching regular expressions"
      ],
      "metadata": {
        "id": "bdFBxtGhNk_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "import random\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "word = \"quick\"\n",
        "print(\"Original Sentence:\", sentence)\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "synonyms = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for synset in wordnet.synsets(word):\n",
        "    for synonym in synset.lemmas():\n",
        "        synonyms.append(synonym.name())\n",
        "new_sentence = \"\"\n",
        "for i in tokens:\n",
        "    if word == i:\n",
        "        new_sentence += synonyms[random.randint(0, len(synonyms) - 1)] + \" \"\n",
        "    else:\n",
        "        new_sentence += i + \" \"\n",
        "print(\"New Sentence:\", new_sentence.strip())\n",
        "text = \"This is a sample text with some words to be replaced. Another line with words.\"\n",
        "regex = r\"\\b(sample)\\b\"\n",
        "replacement = \"hello\"\n",
        "new_text = re.sub(regex, replacement, text)\n",
        "\n",
        "print(f\"Original text:\\n{text}\")\n",
        "print(f\"\\nModified text:\\n{new_text}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYiOwaGINpZV",
        "outputId": "14dea0f7-22e1-4328-8e16-d14db04e5aaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: The quick brown fox jumps over the lazy dog.\n",
            "New Sentence: The fast brown fox jumps over the lazy dog .\n",
            "Original text:\n",
            "This is a sample text with some words to be replaced. Another line with words.\n",
            "\n",
            "Modified text:\n",
            "This is a hello text with some words to be replaced. Another line with words.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXPERIMENT 6**"
      ],
      "metadata": {
        "id": "J8-T1WyTP-g6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " PYTHON PROGRAM TO BUILD BAG OF WORDS MODEL."
      ],
      "metadata": {
        "id": "4UnMSmOPQCCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "documents = [\n",
        "    \"This is the first document.\",\n",
        "    \"This is the second document with some overlap.\",\n",
        "    \"This is the third document with new information.\"\n",
        "]\n",
        "\n",
        "# Create the Bag of Words model\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit the model on the preprocessed documents\n",
        "bow_model = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Print the vocabulary\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "\n",
        "# Print the Bag of Words representation for each document\n",
        "print(\"\\nBag of Words:\")\n",
        "for i, doc in enumerate(documents):\n",
        "    print(f\"{doc}:\\n{bow_model.toarray()[i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAG9NaB4QGgL",
        "outputId": "b38b796d-e6fc-4012-df04-8acf81a12798"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['document' 'first' 'information' 'is' 'new' 'overlap' 'second' 'some'\n",
            " 'the' 'third' 'this' 'with']\n",
            "\n",
            "Bag of Words:\n",
            "This is the first document.:\n",
            "[1 1 0 1 0 0 0 0 1 0 1 0]\n",
            "This is the second document with some overlap.:\n",
            "[1 0 0 1 0 1 1 1 1 0 1 1]\n",
            "This is the third document with new information.:\n",
            "[1 0 1 1 1 0 0 0 1 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXPERIMENT 7**"
      ],
      "metadata": {
        "id": "g6cYmoUeQnWX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " PYTHON PROGRAM TO FIND TF-IDF VALUES OF EACH WORDS IN A DOCUMENT"
      ],
      "metadata": {
        "id": "IfsbkizCQqIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "corpus = [\n",
        "    'data science is one of the most important fields of science',\n",
        "    'this is one of the best data science courses',\n",
        "    'data scientists analyze data'\n",
        "]\n",
        "\n",
        "words_set = set()\n",
        "for doc in corpus:\n",
        "    words_set.update(nltk.word_tokenize(doc.lower()))\n",
        "\n",
        "n_docs, n_words_set = len(corpus), len(words_set)\n",
        "df_tf = pd.DataFrame(np.zeros((n_docs, n_words_set)), columns=list(words_set))\n",
        "\n",
        "for i in range(n_docs):\n",
        "    words = nltk.word_tokenize(corpus[i].lower())\n",
        "    for w in words:\n",
        "        df_tf.at[i, w] += words.count(w) / len(words)\n",
        "\n",
        "idf = {}\n",
        "for w in words_set:\n",
        "    k = sum(1 for i in range(n_docs) if w in nltk.word_tokenize(corpus[i].lower()))\n",
        "    idf[w] = np.log10(n_docs / k) if k > 0 else 0\n",
        "\n",
        "df_tf_idf = df_tf.copy()\n",
        "for w in words_set:\n",
        "    for i in range(n_docs):\n",
        "        df_tf_idf.at[i, w] *= idf[w]\n",
        "\n",
        "print(df_tf)\n",
        "print(idf)\n",
        "print(df_tf_idf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTxtMq6qQ3C4",
        "outputId": "d9061191-3b94-4049-97ec-41bd36f7ed04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         of       one      this  analyze   courses        is      best  \\\n",
            "0  0.363636  0.090909  0.000000     0.00  0.000000  0.090909  0.000000   \n",
            "1  0.111111  0.111111  0.111111     0.00  0.111111  0.111111  0.111111   \n",
            "2  0.000000  0.000000  0.000000     0.25  0.000000  0.000000  0.000000   \n",
            "\n",
            "       most   science      data  important  scientists    fields       the  \n",
            "0  0.090909  0.363636  0.090909   0.090909        0.00  0.090909  0.090909  \n",
            "1  0.000000  0.111111  0.111111   0.000000        0.00  0.000000  0.111111  \n",
            "2  0.000000  0.000000  1.000000   0.000000        0.25  0.000000  0.000000  \n",
            "{'of': np.float64(0.17609125905568124), 'one': np.float64(0.17609125905568124), 'this': np.float64(0.47712125471966244), 'analyze': np.float64(0.47712125471966244), 'courses': np.float64(0.47712125471966244), 'is': np.float64(0.17609125905568124), 'best': np.float64(0.47712125471966244), 'most': np.float64(0.47712125471966244), 'science': np.float64(0.17609125905568124), 'data': np.float64(0.0), 'important': np.float64(0.47712125471966244), 'scientists': np.float64(0.47712125471966244), 'fields': np.float64(0.47712125471966244), 'the': np.float64(0.17609125905568124)}\n",
            "         of       one      this  analyze   courses        is      best  \\\n",
            "0  0.064033  0.016008  0.000000  0.00000  0.000000  0.016008  0.000000   \n",
            "1  0.019566  0.019566  0.053013  0.00000  0.053013  0.019566  0.053013   \n",
            "2  0.000000  0.000000  0.000000  0.11928  0.000000  0.000000  0.000000   \n",
            "\n",
            "       most   science  data  important  scientists    fields       the  \n",
            "0  0.043375  0.064033   0.0   0.043375     0.00000  0.043375  0.016008  \n",
            "1  0.000000  0.019566   0.0   0.000000     0.00000  0.000000  0.019566  \n",
            "2  0.000000  0.000000   0.0   0.000000     0.11928  0.000000  0.000000  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXPERIMENT 8**"
      ],
      "metadata": {
        "id": "G1Ij6znIRViv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.chunk import ne_chunk\n",
        "\n",
        "text = \"GeeksforGeeks is a recognised platform for online learning in India\"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "tagged = pos_tag(tokens)\n",
        "entities = ne_chunk(tagged)\n",
        "\n",
        "print(entities)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JjAgHK4RLpa",
        "outputId": "74f3f371-9d61-49e4-ef45-4e6ba746cdbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (ORGANIZATION GeeksforGeeks/NNP)\n",
            "  is/VBZ\n",
            "  a/DT\n",
            "  recognised/JJ\n",
            "  platform/NN\n",
            "  for/IN\n",
            "  online/NN\n",
            "  learning/NN\n",
            "  in/IN\n",
            "  (GPE India/NNP))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXPERIMENT 9**"
      ],
      "metadata": {
        "id": "UqtNASjERpb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "corpus = [\n",
        "    'data science is one of the most important fields of science',\n",
        "    'this is one of the best data science courses',\n",
        "    'data scientists analyze data'\n",
        "]\n",
        "\n",
        "words_set = set()\n",
        "for doc in corpus:\n",
        "    words_set.update(nltk.word_tokenize(doc.lower()))\n",
        "\n",
        "n_docs, n_words_set = len(corpus), len(words_set)\n",
        "df_tf = pd.DataFrame(np.zeros((n_docs, n_words_set)), columns=list(words_set))\n",
        "\n",
        "for i in range(n_docs):\n",
        "    words = nltk.word_tokenize(corpus[i].lower())\n",
        "    for w in words:\n",
        "        df_tf.at[i, w] += np.log10(words.count(w) + 1)  # Added 1 to avoid log(0)\n",
        "\n",
        "idf = {}\n",
        "for w in words_set:\n",
        "    k = sum(1 for i in range(n_docs) if w in nltk.word_tokenize(corpus[i].lower()))\n",
        "    idf[w] = np.log10(n_docs / k) if k > 0 else 0\n",
        "\n",
        "df_tf_idf = pd.DataFrame(np.zeros((n_docs, n_words_set + 1)), columns=list(words_set) + ['|d|'])\n",
        "for i in range(n_docs):\n",
        "    temp = 0\n",
        "    for w in words_set:\n",
        "        df_tf_idf.at[i, w] = df_tf.at[i, w] * idf[w]\n",
        "        temp += df_tf_idf.at[i, w] ** 2\n",
        "    df_tf_idf.at[i, '|d|'] = np.sqrt(temp)\n",
        "\n",
        "print(\"Scoring the Documents:\")\n",
        "query = input(\"Enter the query: \").strip()\n",
        "token_query = nltk.word_tokenize(query.lower())\n",
        "\n",
        "score = pd.DataFrame(np.zeros((n_docs, len(token_query) + 3)), columns=list(token_query) + ['|d|', 'Score', 'Rank'])\n",
        "for i in range(n_docs):\n",
        "    temp = 0\n",
        "    for w in token_query:\n",
        "        if w in df_tf_idf.columns:\n",
        "            score.at[i, w] = df_tf_idf.at[i, w]\n",
        "            temp += score.at[i, w]\n",
        "    score.at[i, '|d|'] = df_tf_idf.at[i, '|d|']\n",
        "    score.at[i, 'Score'] = round(temp / score.at[i, '|d|'], 3) if score.at[i, '|d|'] != 0 else 0\n",
        "\n",
        "score['Rank'] = score['Score'].rank(ascending=False)\n",
        "score = score.sort_values(by='Rank')\n",
        "\n",
        "print(score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ON-y1IERtzU",
        "outputId": "1d126c68-1ccc-4ab4-d9ff-33edcfc243af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scoring the Documents:\n",
            "Enter the query: DATA \n",
            "   data       |d|  Score  Rank\n",
            "0   0.0  0.356072    0.0   2.0\n",
            "1   0.0  0.275566    0.0   2.0\n",
            "2   0.0  0.203120    0.0   2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXPERIMENT 10**"
      ],
      "metadata": {
        "id": "fdXDRYX-R5LZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MACHINELEARNINGBASEDTEXTCLASSIFICATIONINPYTHON(NAIVEBAYES CLASSIFIER)"
      ],
      "metadata": {
        "id": "2VTr-0iKR8id"
      }
    }
  ]
}